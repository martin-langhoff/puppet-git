#!/bin/bash
# run as puppet-dashboard

if [[ -n "${PPG_DEBUG}" ]]; then
	set -x
fi
PPG_EXEC_PATH=$( dirname $(readlink -f "${BASH_SOURCE[0]}" ) )a=

# Assume reports are around 400KB each uncompressed.
# (this is the full report size, "lazy" reports are small)
# Each batch of 3000 will take 1.2GB of disk space
# and that 1.2GB will remain in use until GC'd 2hs later.
max_per_batch=3000
reportsdir=/var/lib/ppg/reports-import
reportstmpdir=$(mktemp -d --tmpdir=/var/tmp ppg-reports-to-dashboard.XXXXXXX )

# find files, skipping those older than 2 days,
# ordered by mtime...
for fpath in $(find ${reportsdir} -type f -name '*.yaml.gz' -mtime -2 -printf '%T@ %p\n' | sort -n -k 1 | cut -d' ' -f2 | head -n ${max_per_batch}); do
	seenany=yes
	# strip out reportsdir, split out hostname from filename (which is a timestamp)
	fpath=${fpath:$[${#reportsdir}+1]}
	fname=$(basename $fpath)
	hostname=$(dirname $fpath)
	# we use special chars so quoting is important
	mv "${reportsdir}/$fpath" "$reportstmpdir/${hostname}_${fname}"
done
if [ "${seenany}" = 'yes' ]; then
	gzip -q -d $reportstmpdir/*.gz
	pushd /usr/share/puppet-dashboard >/dev/null
	rake RAILS_ENV=production reports:import REPORT_DIR=${reportstmpdir} | grep -v 'ETA:.*Importing:'
	popd >/dev/null
fi

# the rake/import process needs the file to stick around for a while
# and does not delete it when done, so this is fugly
find /var/tmp -type d -name 'ppg-reports-to-dashboard.*' -user puppet-dashboard -mmin +120 -print0 | \
     xargs -0 --no-run-if-empty -L 100 rm -fr 
